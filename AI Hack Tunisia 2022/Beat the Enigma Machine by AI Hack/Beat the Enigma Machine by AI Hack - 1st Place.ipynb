{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d0194df",
   "metadata": {},
   "source": [
    "# Beat the Enigma Machine by AI Hack - 1st Place Solution\n",
    "    By: Mouafak Dakhlaoui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fd3714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cc1fea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 1.3.5\n",
      "Numpy version: 1.23.2\n",
      "Tensorflow version: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "print(f'Pandas version: {pd.__version__}')\n",
    "print(f'Numpy version: {np.__version__}')\n",
    "print(f'Tensorflow version: {tf.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa89421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the folder containing train.csv and test.csv\n",
    "DATA_DIR = 'data'\n",
    "\n",
    "# Create a folder for submissions\n",
    "SUBMISSIONS_DIR = 'submissions'\n",
    "if not os.path.isdir(SUBMISSIONS_DIR):\n",
    "    os.mkdir(SUBMISSIONS_DIR)\n",
    "    \n",
    "# Set seed\n",
    "SEED = 42\n",
    "\n",
    "tf.keras.utils.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a444cb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (56189, 4)\n",
      "Testing data shape: (2495, 3)\n"
     ]
    }
   ],
   "source": [
    "# Read training and testing data\n",
    "train_data = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))\n",
    "test_data = pd.read_csv(os.path.join(DATA_DIR, 'test.csv'))\n",
    "\n",
    "\n",
    "print(f'Training data shape: {train_data.shape}')\n",
    "print(f'Testing data shape: {test_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c293cb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data):\n",
    "    \"\"\"\n",
    "        A function that takes the training dataset as input and returns\n",
    "        our three model inputs\n",
    "    \"\"\"\n",
    "    plain_text = list(data['plain_text'].values)\n",
    "    encryption_key = list(data['encryption_key'].values)\n",
    "    encrypted_text = list(data['encrypted_text'].values)\n",
    "    return encrypted_text, encryption_key, plain_text\n",
    "\n",
    "def train_test_split(data, validation_split=0.1):\n",
    "    encryption, key, decryption = load_data(train_data)\n",
    "    n_val = int(len(encryption) * validation_split)\n",
    "    return encryption[:-n_val], key[:-n_val], decryption[:-n_val], encryption[-n_val:], key[-n_val:], decryption[-n_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f94aeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split = 0.01\n",
    "train_encryption, train_key, train_decryption, val_encryption, val_key, val_decryption = train_test_split(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9ccbb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation tf datasets.\n",
    "BUFFER_SIZE = len(train_encryption)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_encryption, train_key, train_decryption)).shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_encryption, val_key, val_decryption))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84c4c75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use keras' TextVectorization layer to transform the plain (decrypted) text, the key and the encrypted text\n",
    "# into tokens.\n",
    "vocabulary = [chr(index) for index in range(ord('A'), ord('Z') + 1)]\n",
    "output_sequence_length = int(max(train_data['encrypted_text'].map(len).max(), train_data['plain_text'].map(len).max()))\n",
    "\n",
    "encryption_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=None,\n",
    "    split='character',\n",
    "    output_sequence_length=output_sequence_length,\n",
    "    vocabulary=vocabulary)\n",
    "\n",
    "key_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=None,\n",
    "    split='character',\n",
    "    output_sequence_length=3,\n",
    "    vocabulary=vocabulary\n",
    ")\n",
    "\n",
    "decryption_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=None,\n",
    "    split='character',\n",
    "    output_sequence_length=output_sequence_length,\n",
    "    vocabulary=vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "501e82c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Transform string to tokens.\n",
    "train_ds = train_dataset.map(lambda encryption, key, decryption: ((encryption_text_processor(encryption), key_text_processor(key)), decryption_text_processor(decryption)))\n",
    "val_ds = val_dataset.map(lambda encryption, key, decryption: ((encryption_text_processor(encryption), key_text_processor(key)), decryption_text_processor(decryption)))\n",
    "\n",
    "# Optimize the datasets for performance.\n",
    "train_ds = train_ds.cache().prefetch(AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba711082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "\n",
    "encryption_embedding_dim = 128\n",
    "key_embedding_dim = 128\n",
    "\n",
    "key_units = 256\n",
    "\n",
    "decryption_units1 = 256\n",
    "decryption_units2 = 128\n",
    "\n",
    "\n",
    "dense_units1 = 128\n",
    "dense_units2 = 64\n",
    "dense_units3 = 32\n",
    "\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    encrypted_input = tf.keras.layers.Input((output_sequence_length))\n",
    "    key_input = tf.keras.layers.Input((3))\n",
    "    \n",
    "    embedded_encryption = tf.keras.layers.Embedding(encryption_text_processor.vocabulary_size()-1,\n",
    "                                                   encryption_embedding_dim,\n",
    "                                                   mask_zero=True)(encrypted_input)\n",
    "    \n",
    "    embedded_key = tf.keras.layers.Embedding(3,\n",
    "                                            key_embedding_dim)(key_input)\n",
    "    \n",
    "    key1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(key_units))(embedded_key)\n",
    "    key2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(key_units))(embedded_key)\n",
    "    \n",
    "    mapped_key1 = tf.keras.layers.Dense(decryption_units1, activation='relu')(key1)\n",
    "    mapped_key2 = tf.keras.layers.Dense(decryption_units1, activation='relu')(key2)\n",
    "    \n",
    "    mapped_key3 = tf.keras.layers.Dense(decryption_units2, activation='relu')(mapped_key1)\n",
    "    mapped_key4 = tf.keras.layers.Dense(decryption_units2, activation='relu')(mapped_key2)\n",
    "    \n",
    "    decryption1 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(decryption_units1, return_sequences=True))(embedded_encryption, initial_state=[mapped_key1, mapped_key2])\n",
    "    decryption2 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(decryption_units2, return_sequences=True))(decryption1, initial_state=[mapped_key3, mapped_key4])\n",
    "    \n",
    "    pre_output1 = tf.keras.layers.Dense(dense_units1, activation='relu')(decryption2)\n",
    "    pre_output2 = tf.keras.layers.Dense(dense_units2, activation='relu')(pre_output1)\n",
    "    pre_output3 = tf.keras.layers.Dense(dense_units3, activation='relu')(pre_output2)\n",
    "    \n",
    "    output = tf.keras.layers.Dense(decryption_text_processor.vocabulary_size())(pre_output3)\n",
    "    \n",
    "    model = tf.keras.Model([encrypted_input, key_input], output)\n",
    "    \n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True,\n",
    "                                                         reduction='none'),\n",
    "                 optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_final_model(model):\n",
    "    \"\"\"\n",
    "        A function that takes a model as input and adds a sigmoid activation at the output layer\n",
    "    \"\"\"\n",
    "    encrypted_input = tf.keras.layers.Input((output_sequence_length))\n",
    "    key_input = tf.keras.layers.Input((3))\n",
    "    \n",
    "    model_output = model([encrypted_input, key_input])\n",
    "    \n",
    "    activation_output = tf.keras.layers.Activation('sigmoid')(model_output)\n",
    "    \n",
    "    final_model = tf.keras.Model([encrypted_input, key_input], activation_output)\n",
    "    \n",
    "    final_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction='none'),\n",
    "                       optimizer='adam')\n",
    "    \n",
    "    return final_model\n",
    "    \n",
    "\n",
    "    \n",
    "def load_test_data(data):\n",
    "    encryption_key = list(data['encryption_key'].values)\n",
    "    encrypted_text = list(data['encrypted_text'].values)\n",
    "    return encrypted_text, encryption_key\n",
    "\n",
    "def get_test_ds(data):\n",
    "    encrypted_text, encryption_key = load_test_data(data)\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((encrypted_text, encryption_key))\n",
    "    test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "    test_ds = test_dataset.map(lambda encryption, key: ((encryption_text_processor(encryption), key_text_processor(key)),))\n",
    "    return test_ds\n",
    "\n",
    "\n",
    "def get_submission(model, test_ds):\n",
    "    \"\"\"\n",
    "        A function that takes a model and the test dataset as parameters and returns a submission\n",
    "    \"\"\"\n",
    "    prediction = model.predict(test_ds)\n",
    "    columns = ['ID']\n",
    "    columns.extend([f'label_{chr(key)}' for key in range(ord('A'), ord('Z') + 1)])\n",
    "    sub = pd.DataFrame(columns=columns)\n",
    "    for index, text, pred in tqdm(zip(test_data['ID'].values, test_data['encrypted_text'], prediction)):\n",
    "        for idx, letter_dist in zip(range(len(text)), pred):\n",
    "\n",
    "            values_dict = {f'label_{chr(key)}':value for key, value in zip(range(ord('A'), ord('Z') + 1), letter_dist[2:])}\n",
    "            values_dict['ID'] = f'{index}_{idx}'\n",
    "            sub = sub.append(values_dict, ignore_index=True)\n",
    "    return sub\n",
    "\n",
    "test_ds = get_test_ds(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b9c49a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 3, 128)       384         ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 135)]        0           []                               \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 512)          788480      ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, 512)         788480      ['embedding_1[0][0]']            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 135, 128)     3456        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          131328      ['bidirectional[0][0]']          \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 256)          131328      ['bidirectional_1[0][0]']        \n",
      "                                                                                                  \n",
      " bidirectional_2 (Bidirectional  (None, 135, 512)    592896      ['embedding[0][0]',              \n",
      " )                                                                'dense[0][0]',                  \n",
      "                                                                  'dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 128)          32896       ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 128)          32896       ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " bidirectional_3 (Bidirectional  (None, 135, 256)    493056      ['bidirectional_2[0][0]',        \n",
      " )                                                                'dense_2[0][0]',                \n",
      "                                                                  'dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 135, 128)     32896       ['bidirectional_3[0][0]']        \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 135, 64)      8256        ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 135, 32)      2080        ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 135, 28)      924         ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,039,356\n",
      "Trainable params: 3,039,356\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/500\n",
      "396/396 [==============================] - 42s 72ms/step - loss: 0.0330 - val_loss: 0.0269\n",
      "Epoch 2/500\n",
      "396/396 [==============================] - 25s 62ms/step - loss: 0.0270 - val_loss: 0.0263\n",
      "Epoch 3/500\n",
      "396/396 [==============================] - 24s 62ms/step - loss: 0.0263 - val_loss: 0.0257\n",
      "Epoch 4/500\n",
      "396/396 [==============================] - 24s 62ms/step - loss: 0.0252 - val_loss: 0.0236\n",
      "Epoch 5/500\n",
      "396/396 [==============================] - 25s 64ms/step - loss: 0.0217 - val_loss: 0.0199\n",
      "Epoch 6/500\n",
      "396/396 [==============================] - 27s 68ms/step - loss: 0.0188 - val_loss: 0.0178\n",
      "Epoch 7/500\n",
      "396/396 [==============================] - 25s 63ms/step - loss: 0.0170 - val_loss: 0.0167\n",
      "Epoch 8/500\n",
      "396/396 [==============================] - 26s 64ms/step - loss: 0.0155 - val_loss: 0.0153\n",
      "Epoch 9/500\n",
      "396/396 [==============================] - 26s 67ms/step - loss: 0.0139 - val_loss: 0.0140\n",
      "Epoch 10/500\n",
      "396/396 [==============================] - 26s 66ms/step - loss: 0.0125 - val_loss: 0.0130\n",
      "Epoch 11/500\n",
      "396/396 [==============================] - 26s 65ms/step - loss: 0.0114 - val_loss: 0.0121\n",
      "Epoch 12/500\n",
      "396/396 [==============================] - 24s 62ms/step - loss: 0.0104 - val_loss: 0.0114\n",
      "Epoch 13/500\n",
      "396/396 [==============================] - 25s 63ms/step - loss: 0.0095 - val_loss: 0.0110\n",
      "Epoch 14/500\n",
      "396/396 [==============================] - 26s 65ms/step - loss: 0.0088 - val_loss: 0.0107\n",
      "Epoch 15/500\n",
      "396/396 [==============================] - 25s 64ms/step - loss: 0.0083 - val_loss: 0.0117\n",
      "Epoch 16/500\n",
      "396/396 [==============================] - 26s 65ms/step - loss: 0.0082 - val_loss: 0.0100\n",
      "Epoch 17/500\n",
      "396/396 [==============================] - 25s 64ms/step - loss: 0.0078 - val_loss: 0.0104\n",
      "Epoch 18/500\n",
      "396/396 [==============================] - 24s 61ms/step - loss: 0.0075 - val_loss: 0.0095\n",
      "Epoch 19/500\n",
      "396/396 [==============================] - 25s 63ms/step - loss: 0.0067 - val_loss: 0.0095\n",
      "Epoch 20/500\n",
      "396/396 [==============================] - 25s 62ms/step - loss: 0.0070 - val_loss: 0.0097\n",
      "Epoch 21/500\n",
      "396/396 [==============================] - 26s 66ms/step - loss: 0.0067 - val_loss: 0.0091\n",
      "Epoch 22/500\n",
      "396/396 [==============================] - 24s 61ms/step - loss: 0.0061 - val_loss: 0.0092\n",
      "Epoch 23/500\n",
      "396/396 [==============================] - 25s 63ms/step - loss: 0.0058 - val_loss: 0.0089\n",
      "Epoch 24/500\n",
      "396/396 [==============================] - 25s 63ms/step - loss: 0.0056 - val_loss: 0.0088\n",
      "Epoch 25/500\n",
      "396/396 [==============================] - 25s 64ms/step - loss: 0.0053 - val_loss: 0.0091\n",
      "Epoch 26/500\n",
      "396/396 [==============================] - 24s 61ms/step - loss: 0.0056 - val_loss: 0.0088\n",
      "Epoch 27/500\n",
      "396/396 [==============================] - 24s 61ms/step - loss: 0.0055 - val_loss: 0.0089\n",
      "Epoch 28/500\n",
      "396/396 [==============================] - 24s 61ms/step - loss: 0.0052 - val_loss: 0.0089\n",
      "Epoch 29/500\n",
      "396/396 [==============================] - 24s 61ms/step - loss: 0.0048 - val_loss: 0.0084\n",
      "Epoch 30/500\n",
      "396/396 [==============================] - 26s 65ms/step - loss: 0.0044 - val_loss: 0.0088\n",
      "Epoch 31/500\n",
      "396/396 [==============================] - 24s 61ms/step - loss: 0.0051 - val_loss: 0.0090\n",
      "Epoch 32/500\n",
      "396/396 [==============================] - 24s 60ms/step - loss: 0.0048 - val_loss: 0.0087\n",
      "Epoch 33/500\n",
      "396/396 [==============================] - 24s 60ms/step - loss: 0.0046 - val_loss: 0.0082\n",
      "Epoch 34/500\n",
      "396/396 [==============================] - 24s 61ms/step - loss: 0.0042 - val_loss: 0.0081\n",
      "Epoch 35/500\n",
      "396/396 [==============================] - 24s 60ms/step - loss: 0.0040 - val_loss: 0.0081\n",
      "Epoch 36/500\n",
      "396/396 [==============================] - 24s 61ms/step - loss: 0.0042 - val_loss: 0.0082\n",
      "Epoch 37/500\n",
      "396/396 [==============================] - 24s 60ms/step - loss: 0.0059 - val_loss: 0.0086\n",
      "Epoch 38/500\n",
      "396/396 [==============================] - 24s 60ms/step - loss: 0.0048 - val_loss: 0.0082\n",
      "Epoch 39/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396/396 [==============================] - 24s 60ms/step - loss: 0.0040 - val_loss: 0.0078\n",
      "Epoch 40/500\n",
      "396/396 [==============================] - 24s 60ms/step - loss: 0.0038 - val_loss: 0.0081\n",
      "Epoch 41/500\n",
      "396/396 [==============================] - 24s 60ms/step - loss: 0.0035 - val_loss: 0.0083\n",
      "Epoch 42/500\n",
      "396/396 [==============================] - 24s 60ms/step - loss: 0.0036 - val_loss: 0.0084\n",
      "Epoch 43/500\n",
      "396/396 [==============================] - 24s 60ms/step - loss: 0.0048 - val_loss: 0.0083\n",
      "Epoch 44/500\n",
      "396/396 [==============================] - 24s 61ms/step - loss: 0.0045 - val_loss: 0.0078\n",
      "Epoch 45/500\n",
      "396/396 [==============================] - 25s 63ms/step - loss: 0.0037 - val_loss: 0.0077\n",
      "Epoch 46/500\n",
      "396/396 [==============================] - 24s 60ms/step - loss: 0.0037 - val_loss: 0.0084\n",
      "Epoch 47/500\n",
      "396/396 [==============================] - 24s 60ms/step - loss: 0.0038 - val_loss: 0.0077\n",
      "Epoch 48/500\n",
      "396/396 [==============================] - 24s 60ms/step - loss: 0.0039 - val_loss: 0.0081\n",
      "Epoch 49/500\n",
      "396/396 [==============================] - 24s 60ms/step - loss: 0.0041 - val_loss: 0.0080\n",
      "Epoch 50/500\n",
      "396/396 [==============================] - 26s 65ms/step - loss: 0.0036 - val_loss: 0.0075\n",
      "Epoch 51/500\n",
      "396/396 [==============================] - 24s 61ms/step - loss: 0.0031 - val_loss: 0.0076\n",
      "Epoch 52/500\n",
      "396/396 [==============================] - 24s 61ms/step - loss: 0.0028 - val_loss: 0.0078\n",
      "Epoch 53/500\n",
      "396/396 [==============================] - 24s 61ms/step - loss: 0.0032 - val_loss: 0.0080\n",
      "Epoch 54/500\n",
      "396/396 [==============================] - 24s 61ms/step - loss: 0.0034 - val_loss: 0.0077\n",
      "Epoch 55/500\n",
      "396/396 [==============================] - 25s 64ms/step - loss: 0.0031 - val_loss: 0.0075\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20714a75400>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model and use early stopping\n",
    "model = get_model()\n",
    "model.summary()\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(patience=5)\n",
    "\n",
    "epochs = 500\n",
    "model.fit(train_ds.map(lambda x, y: (x, tf.one_hot(y, 28))),\n",
    "          epochs=epochs,\n",
    "          validation_data=val_ds.map(lambda x, y: (x, tf.one_hot(y, 28))),\n",
    "         callbacks=[es_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f580f273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "396/396 [==============================] - 25s 62ms/step - loss: 0.0036 - val_loss: 0.0078\n",
      "Epoch 2/500\n",
      "396/396 [==============================] - 25s 62ms/step - loss: 0.0033 - val_loss: 0.0074\n",
      "Epoch 3/500\n",
      "396/396 [==============================] - 27s 67ms/step - loss: 0.0034 - val_loss: 0.0073\n",
      "Epoch 4/500\n",
      "396/396 [==============================] - 27s 69ms/step - loss: 0.0034 - val_loss: 0.0078\n",
      "Epoch 5/500\n",
      "396/396 [==============================] - 26s 64ms/step - loss: 0.0031 - val_loss: 0.0074\n",
      "Epoch 6/500\n",
      "396/396 [==============================] - 26s 65ms/step - loss: 0.0026 - val_loss: 0.0074\n",
      "Epoch 7/500\n",
      "396/396 [==============================] - 24s 61ms/step - loss: 0.0037 - val_loss: 0.0073\n",
      "Epoch 8/500\n",
      "396/396 [==============================] - 25s 64ms/step - loss: 0.0030 - val_loss: 0.0073\n",
      "Epoch 9/500\n",
      "396/396 [==============================] - 25s 64ms/step - loss: 0.0026 - val_loss: 0.0074\n",
      "Epoch 10/500\n",
      "396/396 [==============================] - 25s 64ms/step - loss: 0.0028 - val_loss: 0.0078\n",
      "Epoch 11/500\n",
      "396/396 [==============================] - 25s 64ms/step - loss: 0.0031 - val_loss: 0.0072\n",
      "Epoch 12/500\n",
      "396/396 [==============================] - 25s 64ms/step - loss: 0.0028 - val_loss: 0.0075\n",
      "Epoch 13/500\n",
      "396/396 [==============================] - 25s 64ms/step - loss: 0.0030 - val_loss: 0.0076\n",
      "Epoch 14/500\n",
      "396/396 [==============================] - 26s 66ms/step - loss: 0.0028 - val_loss: 0.0075\n",
      "Epoch 15/500\n",
      "396/396 [==============================] - 25s 64ms/step - loss: 0.0026 - val_loss: 0.0080\n",
      "Epoch 16/500\n",
      "396/396 [==============================] - 25s 64ms/step - loss: 0.0028 - val_loss: 0.0073\n",
      "Epoch 17/500\n",
      "396/396 [==============================] - 26s 66ms/step - loss: 0.0031 - val_loss: 0.0074\n",
      "Epoch 18/500\n",
      "396/396 [==============================] - 26s 65ms/step - loss: 0.0026 - val_loss: 0.0073\n",
      "Epoch 19/500\n",
      "396/396 [==============================] - 24s 59ms/step - loss: 0.0028 - val_loss: 0.0076\n",
      "Epoch 20/500\n",
      "396/396 [==============================] - 25s 64ms/step - loss: 0.0045 - val_loss: 0.0074\n",
      "Epoch 21/500\n",
      "396/396 [==============================] - 24s 60ms/step - loss: 0.0038 - val_loss: 0.0084\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2077fa12df0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Used early stopping patience=10 and restore_best_weights=True to squeeze more performance out of the model\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "epochs = 500\n",
    "model.fit(train_ds.map(lambda x, y: (x, tf.one_hot(y, 28))),\n",
    "          epochs=epochs,\n",
    "          validation_data=val_ds.map(lambda x, y: (x, tf.one_hot(y, 28))),\n",
    "         callbacks=[es_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "782cb08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 135)]        0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " model (Functional)             (None, 135, 28)      3039356     ['input_3[0][0]',                \n",
      "                                                                  'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 135, 28)      0           ['model[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,039,356\n",
      "Trainable params: 3,039,356\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get the final model (with the sigmoid activation at the output layer)\n",
    "final_model = get_final_model(model)\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66af95a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 5s 33ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2495it [03:50, 10.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# Make a submission\n",
    "sub = get_submission(final_model, test_ds)\n",
    "sub.to_csv(os.path.join(SUBMISSIONS_DIR, 'model5(0.0066).csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
